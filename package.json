{
  "name": "llama.native.js",
  "version": "0.1.1",
  "description": "serve websocket inference models using GGML 4/5bit Quantized LLM's based on Meta's LLaMa model as wrapper for llama.ccp",
  "os": [
    "win32"
  ],
  "cpu": [
    "x64"
  ],
  "main": "bin/index.js",
  "type": "module",
  "scripts": {
    "setup": "npm i --save --include=dev",
    "prestart": "npm run build",
    "start": "npm run serve",
    "build": "tsc -p ./tsconfig.json",
    "dev": "tsc -p ./tsconfig.json --watch",
    "serve": "nodemon ./bin/serve.js",
    "conn": "nodemon ./bin/connect.js",
    "deploy::prod": "vercel --prod || npx vercel --prod",
    "deploy::preview": "vercel || npx vercel; echo \"You can find a preview build at <project>.vercel.app ...\";"
  },
  "keywords": [
    "llm",
    "wrapper",
    "huggingface",
    "javascript",
    "llama.cpp",
    "socket.io",
    "dalai",
    "quantized",
    "cpu",
    "text",
    "generation"
  ],
  "author": "dave <github.com/daveinchy>",
  "license": "MIT",
  "dependencies": {
    "@types/express": "4.17.17",
    "@types/mongodb": "^4.0.7",
    "@types/typescript": "2.0.0",
    "axios": "^1.4.0",
    "bufferutil": "^4.0.7",
    "cors": "^2.8.5",
    "dotenv": "^16.0.3",
    "express": "^4.18.2",
    "fast-stringify": "^2.0.0",
    "fetch": "^1.1.0",
    "fs": "^0.0.1-security",
    "fs-extra": "^11.1.1",
    "fstream": "^1.0.12",
    "mongodb": "^5.5.0",
    "nodemon": "^2.0.22",
    "socket.io": "^4.6.1",
    "socket.io-client": "^4.6.1",
    "socket.io-parser": "^3.4.3",
    "supports-color": "^9.3.1",
    "ts-node": "10.9.1",
    "tslib": "2.5.2",
    "typescript": "5.0.4",
    "utf-8-validate": "^5.0.10"
  },
  "devDependencies": {
    "@types/node": "^20.2.5"
  }
}
