{
  "name": "llama.native.js",
  "version": "1.0.2",
  "description": "use `npm i --save llama.native.js` to run lama.cpp models on your local machine. features a socket.io server and client that can do inference with the host of the model.",
  "cpu": [
    "x64"
  ],
  "main": "dist/client.io.js",
  "type": "module",
  "scripts": {
    "setup": "npm i --save --include=dev",
    "prestart": "npm run build",
    "start": "npm run serve",
    "build": "tsc -p ./tsconfig.json",
    "dev": "tsc -p ./tsconfig.json --watch",
    "serve": "nodemon ./dist/serve.io.js"
  },
  "keywords": [
    "llm",
    "wrapper",
    "huggingface",
    "javascript",
    "llama.cpp",
    "socket.io",
    "dalai",
    "quantized",
    "cpu",
    "text",
    "generation"
  ],
  "author": "space dave <github.com/daveinchy>",
  "license": "IDGAS",
  "dependencies": {
    "axios": "^1.4.0",
    "bufferutil": "^4.0.7",
    "cors": "^2.8.5",
    "dotenv": "^16.0.3",
    "express": "^4.18.2",
    "fast-stringify": "^2.0.0",
    "fetch": "^1.1.0",
    "fs": "^0.0.1-security",
    "fs-extra": "^11.1.1",
    "fstream": "^1.0.12",
    "nodemon": "^2.0.22",
    "socket.io": "^4.6.1",
    "socket.io-client": "^4.6.1",
    "socket.io-parser": "^3.4.3",
    "supports-color": "^9.3.1",
    "ts-node": "10.9.1",
    "tslib": "2.5.2",
    "typescript": "5.0.4",
    "utf-8-validate": "^5.0.10"
  },
  "devDependencies": {
    "@types/express": "4.17.17",
    "@types/node": "^20.2.5"
  }
}
