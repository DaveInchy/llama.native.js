{
  "name": "llama.native.js",
  "version": "0.1.1",
  "description": "serve websocket inference models using GGML 4/5bit Quantized LLM's based on Meta's LLaMa model as wrapper for llama.ccp",
  "os": [
    "win32"
  ],
  "cpu": [
    "x64"
  ],
  "main": "bin/connection.js",
  "type": "module",
  "scripts": {
    "setup": "npm i --save --include=dev",
    "prestart": "npm run build",
    "start": "node ./bin/index.js",
    "build": "tsc -p ./tsconfig.json",
    "dev": "tsc -p ./tsconfig.json --watch",
    "code": "node ./bin/api.js",
    "deploy": "vercel --prod || npx vercel --prod",
    "deploy-preview": "vercel || npx vercel; echo \"You can find a preview build at <project>.vercel.app ...\";"
  },
  "keywords": [
    "llm",
    "wrapper",
    "huggingface",
    "javascript",
    "llama.cpp",
    "socket.io",
    "dalai",
    "quantized",
    "cpu",
    "text",
    "generation"
  ],
  "author": "dave <github.com/daveinchy>",
  "license": "MIT",
  "dependencies": {
    "@types/express": "4.17.17",
    "@types/mongodb": "^4.0.7",
    "@types/node": "20.2.3",
    "@types/typescript": "2.0.0",
    "dotenv": "^16.0.3",
    "express": "4.18.2",
    "fs": "^0.0.1-security",
    "fs-extra": "^11.1.1",
    "fstream": "^1.0.12",
    "mongodb": "^5.5.0",
    "socket.io": "^4.6.1",
    "socket.io-client": "^4.6.1",
    "ts-node": "10.9.1",
    "tslib": "2.5.2",
    "typescript": "5.0.4"
  }
}
